"""
Registry Check Utility (Spam Prevention)
========================================

Verifies that actors (miners/validators) are registered on-chain
before allowing access to the gateway.

This prevents spam from unregistered hotkeys.
"""

import bittensor as bt
from typing import Optional, Tuple
import time
import threading

# Import configuration
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from gateway.config import BITTENSOR_NETWORK, BITTENSOR_NETUID

# Cache for metagraph (epoch-based invalidation)
_metagraph_cache = None
_cache_epoch = None  # Track which epoch the cache is from
_cache_epoch_timestamp = None  # Track when we last calculated the epoch
_cache_lock = threading.Lock()  # For quick cache read/write ONLY (no await inside!)
_fetch_in_progress = False  # Flag to prevent concurrent fetches (async-safe)

# Epoch duration in seconds (360 blocks Ã— 12 seconds/block = 4320 seconds = 72 minutes)
EPOCH_DURATION_SECONDS = 360 * 12

# Async subtensor instance (injected at gateway startup)
_async_subtensor = None


def inject_async_subtensor(async_subtensor):
    """
    Inject async subtensor instance at gateway startup.
    
    Called from main.py lifespan to provide shared AsyncSubtensor instance.
    This eliminates memory leaks and HTTP 429 errors from repeated instance creation.
    
    Args:
        async_subtensor: AsyncSubtensor instance from main.py lifespan
    
    Example:
        # In main.py lifespan:
        async with bt.AsyncSubtensor(network="finney") as async_sub:
            registry_utils.inject_async_subtensor(async_sub)
    """
    global _async_subtensor
    _async_subtensor = async_subtensor
    print(f"âœ… AsyncSubtensor injected into registry utils (network: {_async_subtensor.network})")


async def get_metagraph_async() -> bt.metagraph:
    """
    Get Bittensor metagraph using injected async subtensor (ASYNC VERSION).
    
    Use this from async contexts (FastAPI endpoints, background tasks).
    For sync contexts, use get_metagraph() wrapper.
    
    The metagraph is cached for the duration of the current epoch and refreshed
    when a new epoch begins. This ensures:
    1. Multiple validators requesting leads in the same epoch use the same metagraph
    2. New registrations are picked up at epoch boundaries
    3. Thread-safe access prevents simultaneous fetches
    4. Single async subtensor (NO new instances created)
    
    CRITICAL: Lock is only held for cache read/write, NOT during network fetch!
    This prevents blocking the async event loop.
    
    Returns:
        Metagraph object for the configured subnet
    
    Raises:
        Exception: If async_subtensor not injected or unable to fetch metagraph
    """
    global _metagraph_cache, _cache_epoch, _cache_epoch_timestamp, _fetch_in_progress
    import time
    import asyncio
    
    if _async_subtensor is None:
        raise Exception(
            "AsyncSubtensor not injected - call inject_async_subtensor() first. "
            "This should be done in main.py lifespan."
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: Quick cache check (lock held only for read)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with _cache_lock:
        # Fast path: Check if cache is still valid based on time
        if _metagraph_cache is not None and _cache_epoch_timestamp is not None:
            time_since_cache = time.time() - _cache_epoch_timestamp
            
            # If less than 72 minutes (one epoch) have passed, cache is definitely still valid
            if time_since_cache < EPOCH_DURATION_SECONDS:
                print(f"âœ… Using cached metagraph for epoch {_cache_epoch} ({len(_metagraph_cache.hotkeys)} neurons) - {int(time_since_cache)}s old")
                return _metagraph_cache
        
        # Check if another task is already fetching
        if _fetch_in_progress:
            # Another task is fetching - use cache if available, otherwise wait
            if _metagraph_cache is not None:
                print(f"â³ Metagraph fetch in progress - using existing cache for epoch {_cache_epoch}")
                return _metagraph_cache
            # No cache and fetch in progress - we'll proceed (rare race condition)
        
        # Mark that we're starting a fetch
        _fetch_in_progress = True
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: Get current epoch (OUTSIDE the lock - this is async!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        from gateway.utils.epoch import get_current_epoch_id_async
        current_epoch = await get_current_epoch_id_async()
        
        # Quick check: maybe cache is still valid for this epoch
        with _cache_lock:
            if _metagraph_cache is not None and _cache_epoch == current_epoch:
                _cache_epoch_timestamp = time.time()
                _fetch_in_progress = False
                print(f"âœ… Using cached metagraph for epoch {current_epoch} ({len(_metagraph_cache.hotkeys)} neurons) - refreshed timestamp")
                return _metagraph_cache
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 3: Fetch new metagraph (OUTSIDE the lock - this is async!)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Strategy:
        # - Attempts 1-4: Use shared AsyncSubtensor (fast path, reuses WebSocket)
        # - Attempts 5-8: Use fresh sync Subtensor in thread pool (reliable fallback)
        # 
        # The sync fallback runs in asyncio.to_thread() so it does NOT block the
        # event loop. Gateway continues processing other requests while this runs.
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        max_retries = 8
        switch_to_sync_after = 4  # Switch to sync fallback after 4 async failures
        retry_delay = 2
        timeout_per_attempt = 60  # 60 second timeout per attempt
        last_error = None
        cached_epoch = None
        
        with _cache_lock:
            cached_epoch = _cache_epoch
        
        for attempt in range(1, max_retries + 1):
            use_sync_fallback = attempt > switch_to_sync_after
            
            try:
                if use_sync_fallback:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # SYNC FALLBACK: Fresh Subtensor in thread pool
                    # Runs in separate thread - does NOT block event loop
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    print(f"ğŸ”„ Attempt {attempt}/{max_retries}: Using SYNC FALLBACK (thread pool)")
                    print(f"   AsyncSubtensor may be stale, using fresh Subtensor...")
                    print(f"   Network: {BITTENSOR_NETWORK}, NetUID: {BITTENSOR_NETUID}")
                    print(f"   Timeout: {timeout_per_attempt}s")
                    
                    def fetch_metagraph_sync():
                        """Runs in thread pool - does not block event loop"""
                        subtensor = bt.Subtensor(network=BITTENSOR_NETWORK)
                        return subtensor.metagraph(BITTENSOR_NETUID)
                    
                    # Run sync code in thread pool with timeout
                    metagraph = await asyncio.wait_for(
                        asyncio.to_thread(fetch_metagraph_sync),
                        timeout=timeout_per_attempt
                    )
                    print(f"âœ… Sync fallback succeeded!")
                    print(f"   Network: {BITTENSOR_NETWORK}, NetUID: {BITTENSOR_NETUID}")
                    print(f"   Timeout: {timeout_per_attempt}s")
                    
                    # Update cache and return (don't fall through to async path)
                    with _cache_lock:
                        _metagraph_cache = metagraph
                        _cache_epoch = current_epoch
                        _cache_epoch_timestamp = time.time()
                        _fetch_in_progress = False
                    
                    print(f"âœ… Metagraph cached for epoch {current_epoch}: {len(metagraph.hotkeys)} neurons registered (via sync fallback)")
                    return metagraph
                else:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # ASYNC PATH: Use shared AsyncSubtensor (fast when healthy)
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    if cached_epoch is not None and cached_epoch != current_epoch:
                        print(f"ğŸ”„ Epoch changed: {cached_epoch} â†’ {current_epoch}")
                        print(f"ğŸ”„ Refreshing metagraph for new epoch... (attempt {attempt}/{max_retries})")
                    else:
                        print(f"ğŸ”„ Fetching metagraph for epoch {current_epoch}... (attempt {attempt}/{max_retries})")
                
                print(f"   Network: {BITTENSOR_NETWORK}, NetUID: {BITTENSOR_NETUID}")
                print(f"   Timeout: {timeout_per_attempt}s")
                
                metagraph = await asyncio.wait_for(
                    _async_subtensor.metagraph(netuid=BITTENSOR_NETUID),
                    timeout=timeout_per_attempt
                )
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # STEP 4: Update cache (lock held only for write)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                with _cache_lock:
                    _metagraph_cache = metagraph
                    _cache_epoch = current_epoch
                    _cache_epoch_timestamp = time.time()
                    _fetch_in_progress = False
                
                method = "sync fallback" if use_sync_fallback else "async"
                print(f"âœ… Metagraph cached for epoch {current_epoch}: {len(metagraph.hotkeys)} neurons registered (via {method})")
                return metagraph
            
            except asyncio.TimeoutError:
                last_error = TimeoutError(f"Metagraph fetch timed out after {timeout_per_attempt}s")
                method = "sync fallback" if use_sync_fallback else "async"
                print(f"âš ï¸  Metagraph fetch attempt {attempt}/{max_retries} ({method}) TIMEOUT ({timeout_per_attempt}s)")
                
                if attempt < max_retries:
                    if attempt == switch_to_sync_after:
                        print(f"   â†’ Next attempt will use SYNC FALLBACK (fresh Subtensor in thread pool)")
                    else:
                        print(f"   Retrying in {retry_delay}s...")
                    await asyncio.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 10)  # Exponential backoff, cap at 10s
                    continue
                
            except Exception as e:
                last_error = e
                method = "sync fallback" if use_sync_fallback else "async"
                print(f"âš ï¸  Metagraph fetch attempt {attempt}/{max_retries} ({method}) failed: {e}")
                
                if attempt < max_retries:
                    if attempt == switch_to_sync_after:
                        print(f"   â†’ Next attempt will use SYNC FALLBACK (fresh Subtensor in thread pool)")
                    else:
                        print(f"   Retrying in {retry_delay}s...")
                    await asyncio.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 10)  # Exponential backoff, cap at 10s
                    continue
        
        # All retries exhausted
        print(f"âŒ Error fetching metagraph after {max_retries} attempts: {last_error}")
        print(f"   Async attempts: {switch_to_sync_after}, Sync fallback attempts: {max_retries - switch_to_sync_after}")
        
        # Use fallback cache
        with _cache_lock:
            _fetch_in_progress = False
            if _metagraph_cache is not None:
                print(f"âš ï¸  Using metagraph from previous epoch {_cache_epoch} as fallback")
                print(f"âš ï¸  This may not include validators who registered in epoch {current_epoch}")
                _cache_epoch_timestamp = time.time()
                print(f"âš ï¸  Fallback cache will be used for next 72 minutes (prevents retry spam)")
                return _metagraph_cache
        
        raise Exception(f"Failed to fetch metagraph and no cache available: {last_error}")
    
    except Exception as e:
        # Always clear the fetch flag on any exception
        with _cache_lock:
            _fetch_in_progress = False
        raise


def get_metagraph() -> bt.metagraph:
    """
    Get Bittensor metagraph (SYNC WRAPPER - prefer async version).
    
    DEPRECATED: This creates a temporary event loop. Use get_metagraph_async() from async contexts.
    
    Returns:
        Metagraph object for the configured subnet
    
    Raises:
        RuntimeError: If called from async context (use get_metagraph_async instead)
        Exception: If unable to fetch metagraph
    
    Example:
        >>> metagraph = get_metagraph()
        >>> print(f"Subnet has {len(metagraph.hotkeys)} neurons")
    """
    import asyncio
    
    # Check if we're in an async context
    try:
        loop = asyncio.get_running_loop()
        # We're in async context - this is an error
        raise RuntimeError(
            "Called get_metagraph() from async context. "
            "Use 'await get_metagraph_async()' instead. "
            "This is more efficient and doesn't create a new event loop."
        )
    except RuntimeError as e:
        if "no running event loop" in str(e).lower():
            # We're in sync context - create temp loop and run async version
            return asyncio.run(get_metagraph_async())
        else:
            # Error was from our check above - re-raise it
            raise


async def is_registered_hotkey_async(hotkey: str) -> Tuple[bool, Optional[str]]:
    """
    Check if hotkey is registered on the subnet (ASYNC VERSION).
    
    Use this from async contexts (FastAPI endpoints). For sync, use is_registered_hotkey() wrapper.
    
    Args:
        hotkey: SS58 address of the actor
    
    Returns:
        (is_registered, role) where:
        - is_registered: True if hotkey exists in metagraph
        - role: "validator" if active=True AND permit=True, "miner" otherwise
    """
    try:
        # Get metagraph using async version (cached, no new instance)
        metagraph = await get_metagraph_async()
        
        # Check if hotkey exists in metagraph
        if hotkey not in metagraph.hotkeys:
            print(f"ğŸ” Registry check: {hotkey[:20]}... NOT FOUND in metagraph")
            return False, None
        
        # Get UID for this hotkey
        uid = metagraph.hotkeys.index(hotkey)
        
        # Get neuron attributes
        stake = metagraph.S[uid]
        active = bool(metagraph.active[uid])
        validator_permit = bool(metagraph.validator_permit[uid])
        
        print(f"ğŸ” Registry check for {hotkey[:20]}...")
        print(f"   UID: {uid}")
        print(f"   Stake: {stake:.6f} Ï„")
        print(f"   Active: {active}")
        print(f"   Validator Permit: {validator_permit}")
        
        # This handles validators who haven't set active flag but have significant stake
        STAKE_THRESHOLD = 500000  # 500K TAO minimum for stake-based validator classification
        
        # Validators must have:
        # 1. BOTH active=True AND validator_permit=True (normal path), OR
        # 2. Stake > 500K TAO AND validator_permit=True (temporary stake-based override)
        if (active and validator_permit) or (stake > STAKE_THRESHOLD and validator_permit):
            role = "validator"
            if active and validator_permit:
                print(f"   âœ… Role: VALIDATOR (active=True, permit=True)")
            else:
                print(f"   âœ… Role: VALIDATOR (stake={stake:.0f} Ï„ > {STAKE_THRESHOLD}, permit=True)")
                print(f"      âš ï¸  TEMPORARY: Stake-based classification (active={active})")
        else:
            role = "miner"
            print(f"   âœ… Role: MINER (active={active}, permit={validator_permit}, stake={stake:.0f})")
        
        return True, role
    
    except Exception as e:
        print(f"âŒ Registry check error for {hotkey[:20]}...: {e}")
        return False, None


def is_registered_hotkey(hotkey: str) -> Tuple[bool, Optional[str]]:
    """
    Check if hotkey is registered on the subnet (SYNC WRAPPER).
    
    DEPRECATED: Use is_registered_hotkey_async() from async contexts.
    
    Args:
        hotkey: SS58 address of the actor
    
    Returns:
        (is_registered, role)
    
    Example:
        >>> is_registered, role = is_registered_hotkey("5GNJqR7T...")
        >>> if is_registered:
        >>>     print(f"Hotkey is a {role}")
    """
    try:
        # Get metagraph (cached)
        metagraph = get_metagraph()
        
        # Check if hotkey exists in metagraph
        if hotkey not in metagraph.hotkeys:
            print(f"ğŸ” Registry check: {hotkey[:20]}... NOT FOUND in metagraph")
            return False, None
        
        # Get UID for this hotkey
        uid = metagraph.hotkeys.index(hotkey)
        
        # Get neuron attributes
        stake = metagraph.S[uid]
        # Cast numpy bools to Python bools for consistent display
        active = bool(metagraph.active[uid])
        validator_permit = bool(metagraph.validator_permit[uid])
        
        print(f"ğŸ” Registry check for {hotkey[:20]}...")
        print(f"   UID: {uid}")
        print(f"   Stake: {stake:.6f} Ï„")
        print(f"   Active: {active}")
        print(f"   Validator Permit: {validator_permit}")
        
        # TEMPORARY: Allow high-stake validators even if active=False
        # This handles validators who haven't set active flag but have significant stake
        STAKE_THRESHOLD = 500000  # 500K TAO minimum for stake-based validator classification
        
        # Validators must have:
        # 1. BOTH active=True AND validator_permit=True (normal path), OR
        # 2. Stake > 500K TAO AND validator_permit=True (temporary stake-based override)
        if (active and validator_permit) or (stake > STAKE_THRESHOLD and validator_permit):
            role = "validator"
            if active and validator_permit:
                print(f"   âœ… Role: VALIDATOR (active=True, permit=True)")
            else:
                print(f"   âœ… Role: VALIDATOR (stake={stake:.0f} Ï„ > {STAKE_THRESHOLD}, permit=True)")
                print(f"      âš ï¸  TEMPORARY: Stake-based classification (active={active})")
        else:
            role = "miner"
            print(f"   âœ… Role: MINER (active={active}, permit={validator_permit}, stake={stake:.0f})")
        
        return True, role
    
    except Exception as e:
        print(f"âŒ Registry check error for {hotkey[:20]}...: {e}")
        return False, None


async def get_validator_count_async() -> int:
    """
    Get the number of registered validators (ASYNC VERSION).
    
    Use this from async contexts. For sync, use get_validator_count() wrapper.
    
    Returns:
        Number of validators (neurons with active=True AND validator_permit=True,
        OR stake > 500K TAO AND validator_permit=True)
    """
    try:
        metagraph = await get_metagraph_async()
        
        STAKE_THRESHOLD = 500000  # 500K TAO minimum
        
        # Count neurons that are validators (normal OR stake-based)
        validator_count = sum(
            1 for i in range(len(metagraph.hotkeys))
            if (metagraph.active[i] and metagraph.validator_permit[i]) or 
               (metagraph.S[i] > STAKE_THRESHOLD and metagraph.validator_permit[i])
        )
        
        return validator_count
    
    except Exception as e:
        print(f"âŒ Error getting validator count: {e}")
        return 0


def get_validator_count() -> int:
    """
    Get the number of registered validators (SYNC WRAPPER).
    
    DEPRECATED: Use get_validator_count_async() from async contexts.
    
    Returns:
        Number of validators (neurons with active=True AND validator_permit=True,
        OR stake > 500K TAO AND validator_permit=True)
    
    Example:
        >>> count = get_validator_count()
        >>> print(f"Subnet has {count} validators")
    """
    try:
        metagraph = get_metagraph()
        
        STAKE_THRESHOLD = 500000  # 500K TAO minimum
        
        # Count neurons that are validators (normal OR stake-based)
        validator_count = sum(
            1 for i in range(len(metagraph.hotkeys))
            if (metagraph.active[i] and metagraph.validator_permit[i]) or 
               (metagraph.S[i] > STAKE_THRESHOLD and metagraph.validator_permit[i])
        )
        
        return validator_count
    
    except Exception as e:
        print(f"âŒ Error getting validator count: {e}")
        return 0


async def get_miner_count_async() -> int:
    """
    Get the number of registered miners (ASYNC VERSION).
    
    Use this from async contexts. For sync, use get_miner_count() wrapper.
    
    Returns:
        Number of miners (neurons that are NOT validators)
    """
    try:
        metagraph = await get_metagraph_async()
        
        STAKE_THRESHOLD = 500000  # 500K TAO minimum
        
        # Count neurons that are NOT validators (inverse of validator logic)
        miner_count = sum(
            1 for i in range(len(metagraph.hotkeys))
            if not ((metagraph.active[i] and metagraph.validator_permit[i]) or 
                   (metagraph.S[i] > STAKE_THRESHOLD and metagraph.validator_permit[i]))
        )
        
        return miner_count
    
    except Exception as e:
        print(f"âŒ Error getting miner count: {e}")
        return 0


def get_miner_count() -> int:
    """
    Get the number of registered miners (SYNC WRAPPER).
    
    DEPRECATED: Use get_miner_count_async() from async contexts.
    
    Returns:
        Number of miners (neurons that are NOT validators)
    
    Example:
        >>> count = get_miner_count()
        >>> print(f"Subnet has {count} miners")
    """
    try:
        metagraph = get_metagraph()
        
        STAKE_THRESHOLD = 500000  # 500K TAO minimum
        
        # Count neurons that are NOT validators (inverse of validator logic)
        miner_count = sum(
            1 for i in range(len(metagraph.hotkeys))
            if not ((metagraph.active[i] and metagraph.validator_permit[i]) or 
                   (metagraph.S[i] > STAKE_THRESHOLD and metagraph.validator_permit[i]))
        )
        
        return miner_count
    
    except Exception as e:
        print(f"âŒ Error getting miner count: {e}")
        return 0


def clear_metagraph_cache():
    """
    Clear the metagraph cache.
    
    Forces a fresh fetch on the next get_metagraph() call.
    Useful for testing or when you know the metagraph has changed.
    
    Example:
        >>> clear_metagraph_cache()
        >>> metagraph = get_metagraph()  # Will fetch fresh data
    """
    global _metagraph_cache, _cache_epoch, _cache_epoch_timestamp
    with _cache_lock:
        _metagraph_cache = None
        _cache_epoch = None
        _cache_epoch_timestamp = None
        print("ğŸ—‘ï¸  Metagraph cache cleared")


def warm_metagraph_cache(target_epoch: int) -> bool:
    """
    Proactively fetch and cache the metagraph for a specific epoch.
    
    This is called by the background metagraph warmer task at epoch boundaries
    to ensure the cache is ready before any requests arrive.
    
    Args:
        target_epoch: The epoch to fetch metagraph for
        
    Returns:
        True if fetch succeeded, False otherwise
        
    Example:
        >>> success = warm_metagraph_cache(19163)
        >>> if success:
        ...     print("Cache warmed successfully")
    """
    global _metagraph_cache, _cache_epoch, _cache_epoch_timestamp
    import time
    
    with _cache_lock:
        # Check if cache is already warmed for this epoch
        if _cache_epoch == target_epoch:
            print(f"ğŸ”¥ Cache already warmed for epoch {target_epoch}")
            return True
        
        # Fetch new metagraph with retry logic
        max_retries = 8  # Aggressive retries for background warming (user wants 8 attempts)
        retry_delay = 10  # Initial 10s delay between attempts
        timeout_per_attempt = 60  # 60 second timeout per attempt (user requirement)
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”¥ Warming attempt {attempt + 1}/{max_retries} for epoch {target_epoch}...")
                print(f"   Network: {BITTENSOR_NETWORK}, NetUID: {BITTENSOR_NETUID}")
                print(f"   Timeout: {timeout_per_attempt}s per attempt")
                
                # Fetch metagraph with 60-second timeout enforcement
                # We use concurrent.futures to enforce a hard timeout on the blocking call
                from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
                
                def _fetch_metagraph():
                    """Helper function to fetch metagraph (runs in timeout-enforced thread)"""
                    subtensor = bt.subtensor(network=BITTENSOR_NETWORK)
                    return subtensor.metagraph(netuid=BITTENSOR_NETUID)
                
                # Execute with 60-second timeout
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(_fetch_metagraph)
                    metagraph = future.result(timeout=timeout_per_attempt)  # Hard 60s timeout
                
                # Update cache
                _metagraph_cache = metagraph
                _cache_epoch = target_epoch
                _cache_epoch_timestamp = time.time()
                
                print(f"ğŸ”¥ âœ… Cache warmed for epoch {target_epoch}: {len(metagraph.hotkeys)} neurons")
                return True
                
            except FuturesTimeoutError:
                # Timeout after 60 seconds
                if attempt < max_retries - 1:
                    print(f"ğŸ”¥ âš ï¸  Warming attempt {attempt + 1}/{max_retries} timed out after {timeout_per_attempt}s")
                    print(f"   Retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 30)  # Exponential backoff, cap at 30s
                    continue
                else:
                    print(f"ğŸ”¥ âŒ All {max_retries} warming attempts timed out for epoch {target_epoch}")
                    print(f"ğŸ”¥ âš ï¸  Workflow will continue using epoch {target_epoch - 1} cache as fallback")
                    return False
            except Exception as e:
                # Other errors (network, rate limiting, etc.)
                if attempt < max_retries - 1:
                    print(f"ğŸ”¥ âš ï¸  Warming attempt {attempt + 1}/{max_retries} failed: {e}")
                    print(f"   Retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 30)  # Exponential backoff, cap at 30s
                    continue
                else:
                    print(f"ğŸ”¥ âŒ All {max_retries} warming attempts failed for epoch {target_epoch}: {e}")
                    print(f"ğŸ”¥ âš ï¸  Workflow will continue using epoch {target_epoch - 1} cache as fallback")
                    return False
        
        return False


def print_registry_stats():
    """
    Print statistics about the subnet registry.
    
    Shows total neurons, validators, and miners.
    Useful for debugging and monitoring.
    """
    try:
        metagraph = get_metagraph()
        
        total_neurons = len(metagraph.hotkeys)
        validator_count = get_validator_count()
        miner_count = get_miner_count()
        
        print("=" * 60)
        print("Subnet Registry Statistics")
        print("=" * 60)
        print(f"Network: {BITTENSOR_NETWORK}")
        print(f"Netuid: {BITTENSOR_NETUID}")
        print(f"Total Neurons: {total_neurons}")
        print(f"Validators: {validator_count} ({validator_count/total_neurons*100:.1f}%)")
        print(f"Miners: {miner_count} ({miner_count/total_neurons*100:.1f}%)")
        print(f"Cache Epoch: {_cache_epoch if _cache_epoch is not None else 'Not cached'}")
        print("=" * 60)
    
    except Exception as e:
        print(f"âŒ Error printing registry stats: {e}")


async def get_validator_weights_async(validator_hotkey: str) -> tuple[float, float]:
    """
    Get validator's stake and v_trust from metagraph (ASYNC VERSION).
    
    Use this from async contexts. For sync, use get_validator_weights() wrapper.
    
    This is called during COMMIT phase to snapshot validator weights.
    Critical: Must capture at COMMIT time, not REVEAL time, to prevent gaming.
    
    Args:
        validator_hotkey: Validator's SS58 address
    
    Returns:
        (stake, v_trust): Tuple of (TAO stake, validator trust score)
        Returns (0.0, 0.0) if validator not found
    """
    try:
        metagraph = await get_metagraph_async()
        
        # Find validator's UID
        if validator_hotkey not in metagraph.hotkeys:
            print(f"âš ï¸  Validator {validator_hotkey[:20]}... not found in metagraph")
            return (0.0, 0.0)
        
        uid = metagraph.hotkeys.index(validator_hotkey)
        
        # Get stake (TAO amount)
        stake = float(metagraph.S[uid])
        
        # Get v_trust (validator trust/reputation)
        v_trust = float(metagraph.validator_trust[uid]) if hasattr(metagraph, 'validator_trust') else 0.0
        
        print(f"ğŸ“Š Validator weights for {validator_hotkey[:20]}...")
        print(f"   Stake: {stake:.6f} Ï„")
        print(f"   V-Trust: {v_trust:.6f}")
        
        return (stake, v_trust)
    
    except Exception as e:
        print(f"âŒ Error fetching validator weights: {e}")
        return (0.0, 0.0)


def get_validator_weights(validator_hotkey: str) -> tuple[float, float]:
    """
    Get validator's stake and v_trust from metagraph (SYNC WRAPPER).
    
    DEPRECATED: Use get_validator_weights_async() from async contexts.
    
    Args:
        validator_hotkey: Validator's SS58 address
    
    Returns:
        (stake, v_trust): Tuple of (TAO stake, validator trust score)
    
    Example:
        >>> stake, v_trust = get_validator_weights("5FNVgRnrx...")
        >>> print(f"Stake: {stake:.6f} Ï„, V-Trust: {v_trust:.6f}")
    """
    try:
        metagraph = get_metagraph()
        
        # Find validator's UID
        if validator_hotkey not in metagraph.hotkeys:
            print(f"âš ï¸  Validator {validator_hotkey[:20]}... not found in metagraph")
            return (0.0, 0.0)
        
        uid = metagraph.hotkeys.index(validator_hotkey)
        
        # Get stake (TAO amount)
        stake = float(metagraph.S[uid])
        
        # Get v_trust (validator trust/reputation)
        # This is Bittensor's internal validator trust score (validator_trust in metagraph)
        # Defaults to 0.0 if validator has no trust yet
        v_trust = float(metagraph.validator_trust[uid]) if hasattr(metagraph, 'validator_trust') else 0.0
        
        print(f"ğŸ“Š Validator weights for {validator_hotkey[:20]}...")
        print(f"   Stake: {stake:.6f} Ï„")
        print(f"   V-Trust: {v_trust:.6f}")
        
        return (stake, v_trust)
    
    except Exception as e:
        print(f"âŒ Error fetching validator weights: {e}")
        return (0.0, 0.0)

